{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, SeparableConv2D, MaxPool2D, LeakyReLU, Activation\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam, Nadam, SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/pinarayaz/Jupyter/Mathematical Foundations of Data Science/chest_xray/'\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(16, 8))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i, _set in enumerate(['train', 'val', 'test']):\n",
    "    set_path = data_path + _set\n",
    "    ax[i].imshow(plt.imread(set_path+'/NORMAL/'+os.listdir(set_path+'/NORMAL')[1]), cmap='gray')\n",
    "    ax[i].set_title('Dataset: {}, Class: Normal'.format(_set))\n",
    "    ax[i+3].imshow(plt.imread(set_path+'/PNEUMONIA/'+os.listdir(set_path+'/PNEUMONIA')[1]), cmap='gray')\n",
    "    ax[i+3].set_title('Dataset: {}, Class: Pneumonia'.format(_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(150, 150, 1))\n",
    "# Convolutional Block 1\n",
    "x = Conv2D(filters=16, kernel_size=(4, 4), activation='relu', padding='same')(inputs)\n",
    "x = Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "# Convolutional Block 2\n",
    "x = SeparableConv2D(filters=32, kernel_size=(4, 4), activation='relu', padding='same')(x)\n",
    "x = SeparableConv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "# Convolutional Block 3\n",
    "x = SeparableConv2D(filters=64, kernel_size=(4, 4), activation='relu', padding='same')(x)\n",
    "x = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "# Convolutional Block 4\n",
    "x = SeparableConv2D(filters=128, kernel_size=(4, 4), activation='relu', padding='same')(x)\n",
    "x = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(rate=0.25)(x)\n",
    "# Convolutional Block 5\n",
    "x = SeparableConv2D(filters=256, kernel_size=(4, 4), activation='relu', padding='same')(x)\n",
    "x = SeparableConv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(rate=0.25)(x)\n",
    "# Fully Connected Block and Output\n",
    "x = Flatten()(x)\n",
    "x = Dense(units=512, activation='relu')(x)\n",
    "x = Dropout(rate=0.75)(x)\n",
    "x = Dense(units=128, activation='relu')(x)\n",
    "x = Dropout(rate=0.5)(x)\n",
    "x = Dense(units=64, activation='relu')(x)\n",
    "x = Dropout(rate=0.25)(x)\n",
    "output = Dense(units=2, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define precision, recall and f1 score metrics\n",
    "def precision_metric(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall_metric(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def f1_score_metric(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data generators\n",
    "datagen = ImageDataGenerator()\n",
    "traingenerator = datagen.flow_from_directory(data_path + 'train',\n",
    "                                             target_size=(150, 150),\n",
    "                                             color_mode=\"grayscale\",\n",
    "                                             shuffle=True,\n",
    "                                             seed=1,\n",
    "                                             batch_size=16)\n",
    "\n",
    "valgenerator = datagen.flow_from_directory(data_path + '/val', \n",
    "                                           target_size=(150, 150),\n",
    "                                           color_mode=\"grayscale\",\n",
    "                                           shuffle=True,\n",
    "                                           seed=1,\n",
    "                                           batch_size=16)\n",
    "\n",
    "testgenerator = datagen.flow_from_directory(data_path + '/test',\n",
    "                                            target_size=(150, 150),\n",
    "                                            shuffle=False,\n",
    "                                            color_mode=\"grayscale\",\n",
    "                                            batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model parameters in the following cells (optimizer, loss, metrics)\n",
    "model = Model(inputs=inputs, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 (history1) = Default Adam\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', precision_metric, recall_metric, f1_score_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2 (history2) = Adam + Nesterov (Nadam)\n",
    "model.compile(optimizer='Nadam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', precision_metric, recall_metric, f1_score_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3 (history3) = Default SGD\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', precision_metric, recall_metric, f1_score_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4 (history4) = SGD + Momentum\n",
    "opt = SGD(momentum=0.85)\n",
    "model.compile(optimizer=opt,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', precision_metric, recall_metric, f1_score_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 5 (history5) = SGD + Nesterov\n",
    "opt = SGD(nesterov=True)\n",
    "model.compile(optimizer=opt,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', precision_metric, recall_metric, f1_score_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "checkpoint = ModelCheckpoint(filepath='bw.hdf5', save_best_only=True, save_weights_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2)\n",
    "\n",
    "history = model.fit_generator(traingenerator, \n",
    "                              steps_per_epoch = traingenerator.samples // batch_size,\n",
    "                              epochs = epochs, \n",
    "                              validation_data = testgenerator, \n",
    "                              validation_steps = testgenerator.samples // batch_size,\n",
    "                              callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load best weights\n",
    "model.load_weights('bw.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "no_steps = len(testgenerator)\n",
    "loss, accuracy, precision, recall, f1_score = model.evaluate_generator(testgenerator, \n",
    "                                                                       steps=no_steps, \n",
    "                                                                       verbose=True)\n",
    "print(\"Loss: \" + str(loss))\n",
    "print(\"Accuracy: \" + str(accuracy))\n",
    "print(\"Precision: \" + str(precision))\n",
    "print(\"Recall: \" + str(recall))\n",
    "print(\"F1 Score: \" + str(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 1, figsize=(6, 30))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i, met in enumerate(['loss', 'acc', 'precision_metric', 'recall_metric', 'f1_score_metric']):\n",
    "    ax[i].plot(history.history[met])\n",
    "    ax[i].plot(history.history['val_' + met])\n",
    "    ax[i].set_title('Model {}'.format(met))\n",
    "    ax[i].set_xlabel('epoch')\n",
    "    ax[i].set_ylabel(met)\n",
    "    ax[i].legend(['train', 'val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict_generator(testgenerator, steps = no_steps)\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save and load history\n",
    "import pickle\n",
    "class ModelHistory(object):\n",
    "    def __init__(self, history, epoch, params):\n",
    "        self.history = history\n",
    "        self.epoch = epoch\n",
    "        self.params = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "save_path = '/Users/pinarayaz/Jupyter/Mathematical Foundations of Data Science/history5_v2'\n",
    "with open(save_path, 'wb') as file:\n",
    "    model_history = ModelHistory(history.history, history.epoch, history.params)\n",
    "    pickle.dump(model_history, file, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "path1 = '/Users/pinarayaz/Jupyter/Mathematical Foundations of Data Science/history1_v2'\n",
    "with open(path1, 'rb') as file:\n",
    "    history1 = pickle.load(file)\n",
    "    \n",
    "path2 = '/Users/pinarayaz/Jupyter/Mathematical Foundations of Data Science/history2_v2'\n",
    "with open(path2, 'rb') as file:\n",
    "    history2 = pickle.load(file)\n",
    "    \n",
    "path3 = '/Users/pinarayaz/Jupyter/Mathematical Foundations of Data Science/history3_v2'\n",
    "with open(path3, 'rb') as file:\n",
    "    history3 = pickle.load(file)\n",
    "    \n",
    "path4 = '/Users/pinarayaz/Jupyter/Mathematical Foundations of Data Science/history4_v2'\n",
    "with open(path4, 'rb') as file:\n",
    "    history4 = pickle.load(file)\n",
    "    \n",
    "path5 = '/Users/pinarayaz/Jupyter/Mathematical Foundations of Data Science/history5_v2'\n",
    "with open(path5, 'rb') as file:\n",
    "    history5 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparisons for all models - TRAIN\n",
    "fig, ax = plt.subplots(3, 2, figsize=(16, 20))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i, met in enumerate(['loss', 'acc', 'precision_metric', 'recall_metric', 'f1_score_metric']):\n",
    "    ax[i].plot(history1.history[met])\n",
    "    ax[i].plot(history2.history[met])\n",
    "    ax[i].plot(history3.history[met])\n",
    "    ax[i].plot(history4.history[met])\n",
    "    ax[i].plot(history5.history[met])\n",
    "    \n",
    "    ax[i].set_title('Train {}'.format(met))\n",
    "    ax[i].set_xlabel('epoch')\n",
    "    ax[i].set_ylabel(met)\n",
    "    ax[i].legend(['Adam', 'NAdam', 'SGD', 'SGD + momentum', 'SGD + Nesterov'])\n",
    "    \n",
    "fig.delaxes(ax.flatten()[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparisons for all models - VAL\n",
    "fig, ax = plt.subplots(5, 1, figsize=(6, 30))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i, met in enumerate(['loss', 'acc', 'precision_metric', 'recall_metric', 'f1_score_metric']):\n",
    "    ax[i].plot(history1.history['val_' + met])\n",
    "    ax[i].plot(history2.history['val_' + met])\n",
    "    ax[i].plot(history3.history['val_' + met])\n",
    "    ax[i].plot(history4.history['val_' + met])\n",
    "    ax[i].plot(history5.history['val_' + met])\n",
    "    \n",
    "    ax[i].set_title('Validation {}'.format(met))\n",
    "    ax[i].set_xlabel('epoch')\n",
    "    ax[i].set_ylabel(met)\n",
    "    ax[i].legend(['Adam', 'NAdam', 'SGD', 'SGD + momentum', 'SGD + Nesterov'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
